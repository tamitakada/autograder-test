In recent years, machine learning algorithms have revolutionized the field of artificial intelligence. This paper presents a novel approach to training deep neural networks using a technique called 'layer-wise relevance propagation'. Our experiments demonstrate that this method significantly improves the interpretability of models without compromising their performance. We provide comprehensive results on benchmark datasets to validate our claims.